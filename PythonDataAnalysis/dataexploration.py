# -*- coding: utf-8 -*-
"""DataExploration.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B_UO8haPJLUnShfcTYm6KA5ntTXE0BN7

The dataset analysed below is a collection of medical records collected from two hospitals located in the western part of Kenya by Doctor Little Organisation while conducting medical research in Kakamega county. The county was chosen because of its high prevalence of pediatric morbidity and mortality rates in the country. The hospitals lacked electronic health records; therefore, all medical records were manually entered into an excel file and later converted to a CSV file for analysis. In adherence to the Hippa Compliance, all patient identifiers, including (dates, provider names, dates) were de-identified.

**Data Source:** Iguhu Sub County Hospital and Shibwe Sub-County Hospital

**Country:** Kenya (Kakamega County) 

**Measurement Type(s**): Demographics • clinical measurement • intervention • Insurance  • pharmacotherapy • clinical laboratory test • medical data

**Sample Characteristics:** Children (max 13 years)

**Data Type:** CSV

# **Import Libraries**
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt 
import pandas as pd
import numpy as np
import os
import seaborn as sns

"""# **Data Acquisition**"""

#The dataset is stored locally in a google drive

from google.colab import drive
drive.mount("/content/drive")

# Defining the path to the data directory
Path = '/content/drive/MyDrive/SaaS/AI/ML/'
os.listdir(Path)

"""

> ## **Data Exploration**
"""

data= pd.read_csv("/content/drive/MyDrive/SaaS/AI/ML/Kakamegadataset.csv")

#viewing the first five rows in the data using the head() method
data.head()

#Check the data columns
data.columns

#the dataset contains various pediatric tempatures and not only the highesthence we will change this to avoid confusion
data.rename({'Highest Temperature': "Temperature"}, axis=1, inplace = True)

#descriptive statistical summary of all the dataset columns
data.describe(include ='all')

#using the info() method we print Dataframe information including index dtype, Non-null values and memory usage.
data.info()

#find the missing data
null_data = data.isnull()
null_data.head()

"""#**Count the missing values**"""

#We use a for loop to quickly figure out the number of missing values in each column. 
#the body of the for loop the method ".value_counts()" counts the number of "missing" values

for column in null_data.columns.values.tolist():
    print(column)
    print (null_data[column].value_counts())
    print("")

"""> **Data Summary**

> Based on the summary above:

1. The treatment column has **1** missing data
2. The Labs column has **113** missing data

> **Dealing with Missing Data**
"""

#For this linear model we do not need these two columns therefore we will delete them

data.drop(["Labs", "Treatment"], axis = 1 , inplace =True)
data.head()

"""We then follow by analysing the data to identify any missing values. It is easier to visually analyse the data therefore we will plot the data using the  Seaborn library. """

null_data = data.isna()
sns.heatmap(null_data)

"""

 > # **Exploratory Data Analysis**

The next step is Exploratory Data Analysis (EDA). EDA is the process understanding the data find patterns, relationships, or anomalies to inform our subsequent analysis.in this Notebook we use the pair plots tool (also called a scatterplot matrix) from Seaborn. A pairs plot allows us to see both distribution of single variables and relationships between two variables. Pair plots are a great method to identify trends for follow-up analysis.
"""

sns.set(color_codes=True)
cols = ['Age', "Temperature", 'Onset', "Year"]
sns.pairplot(data[cols], size = 2.5)
plt.show();

"""In order to use linear regression, we will  begin by converting the the independant and dependent variables to binary format using the get_dummy method in pandas to find correlation between them.



"""

#converting all categorical data to binary (1 & 0) format
new_data=pd.get_dummies(data = data, columns=["Outcome", 'Sex',"Hallucinations",	"Rhonchi",	"Crepitations",	"Panic attacks",	"Cold Extremeties",	"Neck stiffness",	"Skin Pustules",	"Peripheral Edema",	"Sweating",	"Eye Discharge",	"Loss of consciousness",	"Chest Indrawing",	"Wheezing",	"Chest Congestion",	"Rhinorrhea",	"GBM",	"Hotness of Body",	"Inability to swallow",	"Feeding Difficulties",	"Chills",	"Headache",	"Loss of Appetite",	"Cough"	,"Convulsion"	,"Abdominal Pain",	"Nausea", "Difficulty in Breathing"	,"Diarrhoea"	,"Dehydration",	"Chest wall indrawing",	"Malnutrition",	"Pallor",	"Irritable",	"Melena",	"Nasal Flaring",	"Vomiting",	"Delayed Milestone",	"Tachypnea"	,"Grunting"	,"Palpitations",	"Abdominal Distention",	"Jaundice",	"Hydrocephalus",	"Oral sores",	"Sick Looking"	,"Positive RDT" ], drop_first=True)
new_data.head()

#we drop all the columns in the dataset above that weren't converted
new_data.drop([	"Age",	"Year",	"Month",	"Onset",	"Temperature",	"Diagnosis"		,"Medical Insuarance",	"Nationality",	"County",	"Residence"], axis = 1, inplace=True)
new_data.head()

#using the concat method we join the previous dataset to the new dataset
dataset= pd.concat([data,new_data], axis=1)
dataset.head()

dataset["Temperature"]

#we then delete all the repeated columns from the dataset that are not in binary form
dataset.drop(['Sex',"Hallucinations",	"Rhonchi",	"Crepitations", 'Chest wall indrawing_Yes',	"Panic attacks",	"Cold Extremeties",	"Neck stiffness",	"Skin Pustules",	"Peripheral Edema",	"Sweating",	"Eye Discharge",	"Loss of consciousness",	"Chest Indrawing",	"Wheezing",	"Chest Congestion",	"Rhinorrhea",	"GBM",	"Hotness of Body",	"Inability to swallow",	"Feeding Difficulties",	"Chills",	"Headache",	"Loss of Appetite",	"Cough"	,"Convulsion"	,"Abdominal Pain",	"Nausea", "Difficulty in Breathing"	,"Diarrhoea"	,"Dehydration",	"Chest wall indrawing",	"Malnutrition",	"Pallor",	"Irritable",	"Melena",	"Nasal Flaring",	"Vomiting",	"Delayed Milestone",	"Tachypnea"	,"Grunting"	,"Palpitations","Outcome",	"Abdominal Distention",	"Jaundice",	"Hydrocephalus",	"Oral sores",	"Sick Looking"	,"Positive RDT"], axis =1 , inplace = True)
dataset.head()

dataset.drop(['Medical Insuarance', 'Nationality',"County", "Residence"], axis = 1, inplace = True)

#finding the correlation between the columns above

data[["Age", "Onset", 'Temperature', 'Year']].corr()

#Plotting a histogram to show age the model's dependent variable.

outcome=data['Temperature']
plt.hist(outcome, color='blue')
plt.title("Temperature")
plt.xlabel("Degrees")
plt.show()

#finding the correlation between the columns above
dataset.corr()

"""# Simple Regression Model"""

#visualising variables using boxplots
sns.boxplot( x= "Positive RDT_Yes", y= "Temperature", data = dataset)

#we do this using the scikit learn library
from sklearn.linear_model import LinearRegression

lm = LinearRegression()
lm

"""> **Traning data distribution**"""

plt.scatter( x= "Age", y= "Temperature",  color='blue', data = dataset)
plt.xlabel("Age")
plt.ylabel("Temperature")
plt.show()

"""
**Let's estimate the model coefficients**

> Coefficient and Intercept in the simple linear regression, are the parameters of the fit line. Coefficients are estimated using the least squares criterion which minimizes the sum of squared residuals (or "sum of squared errors"). 

"""

x = np.asanyarray(dataset[['Age']])
y = np.asanyarray(dataset[['Temperature']])

# instantiate and fit
regr.fit (train_x, train_y)

# The coefficients
print ('Coefficients: ', regr.coef_)
print ('Intercept: ',regr.intercept_)

"""

>  # **Plotting the Results**


"""

#We can plot the fit line over the data:

plt.scatter(x="Age", y="Temperature", data=dataset,  color='blue')
plt.plot(x, regr.coef_[0][0]*x + regr.intercept_[0], '-r')
plt.xlabel("Age")
plt.ylabel("Temperature")

"""# **Interpreting Model Coefficients**

The coefficient value represents the mean change in the response given a one unit change in the predictor. In this case the coefficient is -0.05, the mean response value decreases by the coefficient value for every one unit change in temperature. Since our coerr is almost zero, the mean temperature wouldn't change much no matter how far along the line we move.

> **Residual Plot**

Residual is the difference between the observed value and the predicted value. When we look at a regression plot, the residual is the distance from the data point to the fitted regression line.
"""

#visualize the variance of the data 
width = 12
height = 10
plt.figure(figsize=(width, height))
sns.residplot(dataset['Age'], dataset['Temperature'])
plt.show()

""">  **Conclusion** 
We can see from the residual plot that the residuals are randomly spread around the x- axis, meaning that the variance is constant. The simple regression  model has not fitted very well to the existing data. This suggests that linear regression is not suitable in this case.

> # **Multiple Linear Regression**

Using Multiple Linear Regression we explain the relationship between one continuous response (dependent) variable and two or more predictor (independent) variables. Our goal is to pick symptoms from the dataset that are predicators for High temperature 

Below are good predictor from the dataset for high fever:
 
*  Rhonchi, Crepitations, Neck stiffness, Wheezing,
Chest Congestion, Sweating, Feeding Difficulties, Chills, Headache, Cough, Difficulty in Breathing, Nasal Flaring, Tachypnea, Grunting,Pallor, Sick Looking, Irritable, Positive RDT, Convulsions Hotness of Body and Feeding difficulties.
"""

z= array(dataset[['Rhonchi_Yes', 'Crepitations_Yes',
        'Cold Extremeties_Yes', 'Neck stiffness_Yes', 'Sweating_Yes',
        'Loss of consciousness_Yes',
        'Wheezing_Yes', 'Chest Congestion_yes', 'Hotness of Body_Yes',
        'Feeding Difficulties_Yes', 'Chills_Yes',
        'Cough_Yes', 'Convulsion_Yes',
        'Nausea_Yes', 'Difficulty in Breathing_Yes',
        'Dehydration_Yes',
        'Pallor_Yes', 'Irritable_Yes', 
        'Nasal Flaring_Yes',
        'Tachypnea_Yes', 'Grunting_Yes', 'Palpitations_Yes',
        'Sick Looking_Yes', 'Positive RDT_Yes']])

"""# **Prediction**


"""

#fit the model
lm.fit(z, dataset['Temperature'])

# Find the R^2
print('The R-square is: ', lm.score(z, dataset['Temperature']))

"""We can say that ~25.24 % of the variation of temperature is explained by this multiple linear regression."""

lm.intercept_

lm.coef_

from sklearn.metrics import mean_squared_error

#Let's calculate the MSE.
Y_predict_multifit = lm.predict(z)

print('The mean square error and predicted value using multifit is: ', \
      mean_squared_error(dataset['Temperature'], Y_predict_multifit))

""" > # **Model Evaluation Using Visualization**

 In order to way to look at the fit of the model we visualize distribution plot. We look at the distribution of the fitted values that result from the model and compare it to the distribution of the actual values.
"""

Y_hat = lm.predict(z)

plt.figure(figsize=(width, height))

ax1 = sns.distplot(dataset['Temperature'], hist=False, color="r", label="Actual Value")
sns.distplot(Y_hat, hist=False, color="b", label="Fitted Values" , ax=ax1)

plt.title('Actual vs Fitted Values for Temperature')
plt.xlabel('Temperaure (in degrees)')
plt.ylabel('Symptom Propotion')

plt.show()
plt.close()

"""> We can see that the Multiple regression model performs a bit better than the simple linear regression. However there is room for improvements."""